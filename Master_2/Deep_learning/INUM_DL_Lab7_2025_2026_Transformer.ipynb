{"cells":[{"cell_type":"markdown","id":"1ae38945-39dd-45dc-ad4f-da7a4404241f","metadata":{"id":"1ae38945-39dd-45dc-ad4f-da7a4404241f"},"source":["# Lab 7: Transformer"]},{"cell_type":"markdown","source":["The goal of this lab is to develop a class that implements a multihead attention class. To do this, we will follow several steps:\n","1) Self-attention\n","2) Causal self-attention\n","3) Single head attention class\n","4) Multi-head attention class."],"metadata":{"id":"M4bHmxJ8g34i"},"id":"M4bHmxJ8g34i"},{"cell_type":"markdown","id":"c29bcbe8-a034-43a2-b557-997b03c9882d","metadata":{"id":"c29bcbe8-a034-43a2-b557-997b03c9882d"},"source":["Packages that are being used in this notebook:"]},{"cell_type":"code","execution_count":null,"id":"e58f33e8-5dc9-4dd5-ab84-5a011fa11d92","metadata":{"id":"e58f33e8-5dc9-4dd5-ab84-5a011fa11d92"},"outputs":[],"source":["from importlib.metadata import version\n","\n","print(\"torch version:\", version(\"torch\"))\n","\n","import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","source":["Let us generated an arbitrary embedded sentence. Each word is embedded into a 3-dimensional vector representation."],"metadata":{"id":"OvGUJYZeKCvP"},"id":"OvGUJYZeKCvP"},{"cell_type":"code","source":["inputs = torch.tensor(\n","  [[0.43, 0.15, 0.89], # Your     (x^1)\n","   [0.55, 0.87, 0.66], # journey  (x^2)\n","   [0.57, 0.85, 0.64], # starts   (x^3)\n","   [0.22, 0.58, 0.33], # with     (x^4)\n","   [0.77, 0.25, 0.10], # one      (x^5)\n","   [0.05, 0.80, 0.55]] # step     (x^6)\n",")"],"metadata":{"id":"a9zhAqgtNVsP"},"id":"a9zhAqgtNVsP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_2 = inputs[1] # second input element\n","n_inputs = inputs.shape[0] # the number of inputs\n","d_in = inputs.shape[1] # the input embedding size, d=3\n","d_out = 2 # the output embedding size, d=2"],"metadata":{"id":"YkEe4nbuNsFD"},"id":"YkEe4nbuNsFD","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"a303b6fb-9f7e-42bb-9fdb-2adabf0a6525","metadata":{"id":"a303b6fb-9f7e-42bb-9fdb-2adabf0a6525"},"source":["## 1- Implementing self-attention with trainable weights"]},{"cell_type":"markdown","source":["### Question 1: Implement the forward function in the following class.\n","\n","You must compute:\n","- the keys\n","- the queries\n","- the values\n","- the attention scores\n","- the attention weights\n","- the context vector.\n","\n","The forward function must return the context vector."],"metadata":{"id":"6_UJtX4iMErz"},"id":"6_UJtX4iMErz"},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","\n","    def __init__(self, d_in, d_out, qkv_bias=False):\n","        super().__init__()\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","\n","    def forward(self, x):\n","        # Complete the code\n","\n","\n","        return context_vec\n"],"metadata":{"id":"4-o45L6GViQ3"},"id":"4-o45L6GViQ3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test the SelfAttention class"],"metadata":{"id":"SKFTI1RyV3B_"},"id":"SKFTI1RyV3B_"},{"cell_type":"code","source":["torch.manual_seed(789)\n","sa = SelfAttention(d_in, d_out)\n","print(sa(inputs))"],"metadata":{"id":"wMQkG-s4Njg1"},"id":"wMQkG-s4Njg1","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c5025b37-0f2c-4a67-a7cb-1286af7026ab","metadata":{"id":"c5025b37-0f2c-4a67-a7cb-1286af7026ab"},"source":["## 2- Hiding future words with causal attention"]},{"cell_type":"markdown","id":"82f405de-cd86-4e72-8f3c-9ea0354946ba","metadata":{"id":"82f405de-cd86-4e72-8f3c-9ea0354946ba"},"source":["### 2.1 Applying a causal attention mask"]},{"cell_type":"markdown","id":"014f28d0-8218-48e4-8b9c-bdc5ce489218","metadata":{"id":"014f28d0-8218-48e4-8b9c-bdc5ce489218"},"source":["- We are converting the previous self-attention mechanism into a causal self-attention mechanism\n","- Causal self-attention ensures that the model's prediction for a certain position in a sequence is only dependent on the known outputs at previous positions, not on future positions\n","- In simpler words, this ensures that each next word prediction should only depend on the preceding words\n","- To achieve this, for each given token, we mask out the future tokens (the ones that come after the current token in the input text)."]},{"cell_type":"code","execution_count":null,"id":"1933940d-0fa5-4b17-a3ce-388e5314a1bb","metadata":{"id":"1933940d-0fa5-4b17-a3ce-388e5314a1bb"},"outputs":[],"source":["# Reuse the query and key weight matrices of the\n","# SelfAttention object from the previous section for convenience\n","queries = sa.W_query(inputs)\n","keys = sa.W_key(inputs)\n","attn_scores = queries @ keys.T\n","\n","print(attn_scores)\n","\n","attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","\n","print(attn_weights)"]},{"cell_type":"markdown","source":["### Question 2: We can mask the unnormalized attention scores above the diagonal with negative infinity before they enter the softmax function. Generate an upper triangular matrix of ones and use it to replace the attention scores with negative infinity. Compute the masked attention weights. Hence, you should compute two elements in the following cell: the mask and the masked attention weights."],"metadata":{"id":"7sidtWvAP_Hw"},"id":"7sidtWvAP_Hw"},{"cell_type":"code","source":["context_length = attn_scores.shape[0]\n","\n","# Complete the code"],"metadata":{"id":"ywN2jSLXWKFS"},"id":"ywN2jSLXWKFS","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"7636fc5f-6bc6-461e-ac6a-99ec8e3c0912","metadata":{"id":"7636fc5f-6bc6-461e-ac6a-99ec8e3c0912"},"source":["### 2.2 Masking additional attention weights with dropout"]},{"cell_type":"markdown","id":"ec3dc7ee-6539-4fab-804a-8f31a890c85a","metadata":{"id":"ec3dc7ee-6539-4fab-804a-8f31a890c85a"},"source":["- In addition, we also apply dropout to reduce overfitting during training\n","- We will apply the dropout mask after computing the attention weights because it's more common\n","- In this example, we use a dropout rate of 50%, which means randomly masking out half of the attention weights (with the GPT model later, the dropout rate is 0.1 or 0.2).\n","\n"]},{"cell_type":"markdown","source":["Let us test the dropout"],"metadata":{"id":"LgbOr5e4WrqJ"},"id":"LgbOr5e4WrqJ"},{"cell_type":"code","execution_count":null,"id":"0de578db-8289-41d6-b377-ef645751e33f","metadata":{"id":"0de578db-8289-41d6-b377-ef645751e33f"},"outputs":[],"source":["torch.manual_seed(673)\n","dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n","example = torch.ones(6, 6) # create a matrix of ones\n","\n","print(dropout(example))"]},{"cell_type":"markdown","source":["Important note:\n","- If we apply a dropout rate of 0.5 (50%), the non-dropped values are scaled accordingly by a factor of 1/0.5 = 2\n","- The scaling is calculated by the formula 1 / (1 - `dropout_rate`)\n","- Dropout is only applied during training, not during inference"],"metadata":{"id":"vFb_za8zWq4V"},"id":"vFb_za8zWq4V"},{"cell_type":"markdown","source":["### Question 3: Apply the dropout to the attention weights and print the result."],"metadata":{"id":"x3bBPXGcSTS_"},"id":"x3bBPXGcSTS_"},{"cell_type":"code","source":["torch.manual_seed(673)\n","# Complete the code\n"],"metadata":{"id":"HbvsvXxNdjut"},"id":"HbvsvXxNdjut","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"cdc14639-5f0f-4840-aa9d-8eb36ea90fb7","metadata":{"id":"cdc14639-5f0f-4840-aa9d-8eb36ea90fb7"},"source":["## 3- Implementing a compact causal self-attention class"]},{"cell_type":"markdown","id":"09c41d29-1933-43dc-ada6-2dbb56287204","metadata":{"id":"09c41d29-1933-43dc-ada6-2dbb56287204"},"source":["- Now, we are ready to implement a working implementation of self-attention, including the causal and dropout masks\n","- One more thing is to implement the code to handle batches consisting of more than one input so that our `CausalAttention` class supports the batch outputs produced by the data loader we implemented in chapter 2\n","- For simplicity, to simulate such batch input, we duplicate the input text example:"]},{"cell_type":"code","execution_count":null,"id":"977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28","metadata":{"id":"977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28"},"outputs":[],"source":["batch = torch.stack((inputs, inputs), dim=0)\n","print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"]},{"cell_type":"markdown","source":["### Question 4: in the following class, why \"keys.T\" is replaced with \"keys.transpose(1, 2)\"?"],"metadata":{"id":"a0UuliXpb-kv"},"id":"a0UuliXpb-kv"},{"cell_type":"markdown","source":["Answer: write down your answer here."],"metadata":{"id":"8QOGQnG9XUjZ"},"id":"8QOGQnG9XUjZ"},{"cell_type":"code","execution_count":null,"id":"60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0","metadata":{"id":"60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0"},"outputs":[],"source":["class CausalAttention(nn.Module):\n","\n","    def __init__(self, d_in, d_out, context_length,\n","                 dropout, qkv_bias=False):\n","        super().__init__()\n","        self.d_out = d_out\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.dropout = nn.Dropout(dropout)\n","        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n","        # Used for tensors that need to be on the same device as the module.\n","        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape # batch dimension b\n","        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n","        # in the mask creation further below.\n","        # In practice, this is not a problem since the LLM ensures that inputs\n","        # do not exceed `context_length` before reaching this forward method.\n","        keys = self.W_key(x)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        attn_scores = queries @ keys.transpose(1, 2)\n","        attn_scores.masked_fill_(  # _ ops are in-place\n","            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n","        attn_weights = torch.softmax(\n","            attn_scores / keys.shape[-1]**0.5, dim=-1\n","        )\n","        attn_weights = self.dropout(attn_weights)\n","\n","        context_vec = attn_weights @ values\n","        return context_vec\n","\n"]},{"cell_type":"markdown","source":["Let us test the CausalAttention class."],"metadata":{"id":"YbQCZm0_Xl9_"},"id":"YbQCZm0_Xl9_"},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","context_length = batch.shape[1]\n","ca = CausalAttention(d_in, d_out, context_length, 0.0)\n","\n","context_vecs = ca(batch)\n","\n","print(context_vecs)\n","print(\"context_vecs.shape:\", context_vecs.shape)"],"metadata":{"id":"RlRafU4ddjdM"},"id":"RlRafU4ddjdM","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c8bef90f-cfd4-4289-b0e8-6a00dc9be44c","metadata":{"id":"c8bef90f-cfd4-4289-b0e8-6a00dc9be44c"},"source":["## 4- Extending single-head attention to multi-head attention"]},{"cell_type":"markdown","id":"11697757-9198-4a1c-9cee-f450d8bbd3b9","metadata":{"id":"11697757-9198-4a1c-9cee-f450d8bbd3b9"},"source":["### 4.1 Stacking multiple single-head attention layers"]},{"cell_type":"markdown","id":"70766faf-cd53-41d9-8a17-f1b229756a5a","metadata":{"id":"70766faf-cd53-41d9-8a17-f1b229756a5a"},"source":["- The self-attention implemented previously is also called single-head attention.\n","- We simply stack multiple single-head attention modules to obtain a multi-head attention module\n","- The main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections.\n","- This allows the model to jointly attend to information from different representation subspaces at different positions."]},{"cell_type":"markdown","source":["### Question 5: in the following class, the heads must be concatenated in the \"forward\" function. Code this concatenation."],"metadata":{"id":"rTe945VxfzGh"},"id":"rTe945VxfzGh"},{"cell_type":"code","source":["class MultiHeadAttentionWrapper(nn.Module):\n","\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        self.heads = nn.ModuleList(\n","            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n","             for _ in range(num_heads)]\n","        )\n","\n","    def forward(self, x):\n","        # Write the return line with a concatenation\n","        pass"],"metadata":{"id":"_DVtx0G1gHpn"},"id":"_DVtx0G1gHpn","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let us test the MultiHeadAttentionWrapper class."],"metadata":{"id":"8r0O3gnSYECS"},"id":"8r0O3gnSYECS"},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","context_length = batch.shape[1] # This is the number of tokens\n","d_in, d_out = 3, 2\n","mha = MultiHeadAttentionWrapper(\n","    d_in, d_out, context_length, 0.0, num_heads=2\n",")\n","\n","context_vecs = mha(batch)\n","\n","print(context_vecs)\n","print(\"context_vecs.shape:\", context_vecs.shape)"],"metadata":{"id":"hCu2kQ7_eEQU"},"id":"hCu2kQ7_eEQU","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"193d3d2b-2578-40ba-b791-ea2d49328e48","metadata":{"id":"193d3d2b-2578-40ba-b791-ea2d49328e48"},"source":["- In the implementation above, the embedding dimension is 4, because we `d_out=2` as the embedding dimension for the key, query, and value vectors as well as the context vector. And since we have 2 attention heads, we have the output embedding dimension 2*2=4"]},{"cell_type":"markdown","id":"6836b5da-ef82-4b4c-bda1-72a462e48d4e","metadata":{"id":"6836b5da-ef82-4b4c-bda1-72a462e48d4e"},"source":["### 4.2 Implementing multi-head attention with weight splits"]},{"cell_type":"markdown","id":"f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7","metadata":{"id":"f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7"},"source":["- While the above is an intuitive and fully functional implementation of multi-head attention (wrapping the single-head attention `CausalAttention` implementation from earlier), we can write a stand-alone class called `MultiHeadAttention` to achieve the same\n","\n","- This is essentially a rewritten version of `MultiHeadAttentionWrapper` that is more efficient"]},{"cell_type":"markdown","source":["### Question 6: why is this second implememtation (see the cell below) more efficient that `MultiHeadAttentionWrapper`?"],"metadata":{"id":"sCVIduuBfIZ2"},"id":"sCVIduuBfIZ2"},{"cell_type":"markdown","source":["Answer: write down your answer here."],"metadata":{"id":"EgnTFDMpYLVZ"},"id":"EgnTFDMpYLVZ"},{"cell_type":"code","execution_count":null,"id":"110b0188-6e9e-4e56-a988-10523c6c8538","metadata":{"id":"110b0188-6e9e-4e56-a988-10523c6c8538"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,\n","        # this will result in errors in the mask creation further below.\n","        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n","        # do not exceed `context_length` before reaching this forwar\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec\n","\n"]},{"cell_type":"markdown","source":["Let us test the MultiHeadAttention class."],"metadata":{"id":"F-w_-VLlYXsw"},"id":"F-w_-VLlYXsw"},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","batch_size, context_length, d_in = batch.shape\n","d_out = 2\n","mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n","\n","context_vecs = mha(batch)\n","\n","print(context_vecs)\n","print(\"context_vecs.shape:\", context_vecs.shape)"],"metadata":{"id":"KUOz07v4eUMn"},"id":"KUOz07v4eUMn","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"d334dfb5-2b6c-4c33-82d5-b4e9db5867bb","metadata":{"id":"d334dfb5-2b6c-4c33-82d5-b4e9db5867bb"},"source":["\n","- Both are fully functional implementations that can be used in a GPT class\n","- Note that we added a linear projection layer (`self.out_proj `) to the `MultiHeadAttention` class above as used in the usual definition of the Multi-head attention mecanism."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}