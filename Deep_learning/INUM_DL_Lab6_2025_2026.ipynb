{"cells":[{"cell_type":"markdown","id":"25aa40e3-5109-433f-9153-f5770531fe94","metadata":{"id":"25aa40e3-5109-433f-9153-f5770531fe94"},"source":["# Lab 6: Text Embedding"]},{"cell_type":"markdown","source":["The goal of this lab is to study text embedding. To do this, we will follow several steps:\n","1) Tokenization\n","2) Token IDs (integer identifiers)\n","3) Special tokens\n","4) A famous tokenizer: Byte-Pair Encoding (BPE)\n","5) Data sampling\n","6) Token Embedding\n","7) Positional embedding."],"metadata":{"id":"JmU3N6Lbjqa7"},"id":"JmU3N6Lbjqa7"},{"cell_type":"markdown","id":"76d5d2c0-cba8-404e-9bf3-71a218cae3cf","metadata":{"id":"76d5d2c0-cba8-404e-9bf3-71a218cae3cf"},"source":["Packages that are being used in this notebook:"]},{"cell_type":"code","execution_count":null,"id":"4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3","metadata":{"id":"4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3"},"outputs":[],"source":["from importlib.metadata import version\n","\n","print(\"torch version:\", version(\"torch\"))\n","print(\"tiktoken version:\", version(\"tiktoken\"))\n","\n","import os\n","import urllib.request\n","\n","import re\n","# This module provides regular expression matching operations similar to those found in Perl.\n","\n","import importlib\n","import tiktoken\n","# tiktoken is a fast tokeniser for use with OpenAI's models.\n","print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n","\n","import torch\n","print(\"PyTorch version:\", torch.__version__)\n","\n","from torch.utils.data import Dataset, DataLoader\n"]},{"cell_type":"markdown","id":"2417139b-2357-44d2-bd67-23f5d7f52ae7","metadata":{"id":"2417139b-2357-44d2-bd67-23f5d7f52ae7"},"source":["## 1- Breaking text into small units"]},{"cell_type":"markdown","id":"f9c90731-7dc9-4cd3-8c4a-488e33b48e80","metadata":{"id":"f9c90731-7dc9-4cd3-8c4a-488e33b48e80"},"source":["- In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters\n","- Load raw text we want to work with\n","- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) is a public domain short story"]},{"cell_type":"code","execution_count":null,"id":"40f9d9b1-6d32-485a-825a-a95392a86d79","metadata":{"id":"40f9d9b1-6d32-485a-825a-a95392a86d79"},"outputs":[],"source":["file_path = \"the-verdict.txt\"\n","if not os.path.exists(file_path):\n","    url = (\"https://raw.githubusercontent.com/rasbt/\"\n","           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n","           \"the-verdict.txt\")\n","    urllib.request.urlretrieve(url, file_path)"]},{"cell_type":"markdown","source":["Read the text file"],"metadata":{"id":"DQ4omP9fF5qr"},"id":"DQ4omP9fF5qr"},{"cell_type":"code","execution_count":null,"id":"8a769e87-470a-48b9-8bdb-12841b416198","metadata":{"id":"8a769e87-470a-48b9-8bdb-12841b416198"},"outputs":[],"source":["with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","    raw_text = f.read()\n","\n","print(\"Total number of character:\", len(raw_text))\n","print(raw_text[:99])"]},{"cell_type":"markdown","id":"9b971a46-ac03-4368-88ae-3f20279e8f4e","metadata":{"id":"9b971a46-ac03-4368-88ae-3f20279e8f4e"},"source":["- The goal is to tokenize and embed this text for an LLM\n","- The following regular expression will split on whitespaces. It is tested on a small text."]},{"cell_type":"code","execution_count":null,"id":"ed3a9467-04b4-49d9-96c5-b8042bcf8374","metadata":{"id":"ed3a9467-04b4-49d9-96c5-b8042bcf8374"},"outputs":[],"source":["text = \"Hello, world. Is this-- a test?\"\n","\n","# We don't only want to split on whitespaces but also commas and periods, so let's modify the regular expression to do that as well\n","result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","\n","# Strip whitespace from each item and then filter out any empty strings.\n","# The strip() method removes any leading, and trailing whitespaces.\n","result = [item.strip() for item in result if item.strip()]\n","print(result)"]},{"cell_type":"markdown","id":"5bbea70b-c030-45d9-b09d-4318164c0bb4","metadata":{"id":"5bbea70b-c030-45d9-b09d-4318164c0bb4"},"source":["- This is pretty good, and we are now ready to apply this tokenization to the raw text"]},{"cell_type":"code","execution_count":null,"id":"8c567caa-8ff5-49a8-a5cc-d365b0a78a99","metadata":{"id":"8c567caa-8ff5-49a8-a5cc-d365b0a78a99"},"outputs":[],"source":["preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","print(preprocessed[:30])\n","print(len(preprocessed)) # total number of tokens"]},{"cell_type":"markdown","id":"0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231","metadata":{"id":"0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231"},"source":["## 2- Converting tokens into token IDs"]},{"cell_type":"markdown","id":"b5973794-7002-4202-8b12-0900cd779720","metadata":{"id":"b5973794-7002-4202-8b12-0900cd779720"},"source":["- From these tokens, we can now build a vocabulary that consists of all the unique tokens"]},{"cell_type":"code","execution_count":null,"id":"7fdf0533-5ab6-42a5-83fa-a3b045de6396","metadata":{"id":"7fdf0533-5ab6-42a5-83fa-a3b045de6396"},"outputs":[],"source":["# set: remove the duplicates\n","all_words = sorted(set(preprocessed))\n","vocab_size = len(all_words)\n","\n","print(vocab_size)"]},{"cell_type":"markdown","id":"a5204973-f414-4c0d-87b0-cfec1f06e6ff","metadata":{"id":"a5204973-f414-4c0d-87b0-cfec1f06e6ff"},"source":["- Next, we convert the text tokens into token IDs that we can process via embedding layers later"]},{"cell_type":"code","execution_count":null,"id":"77d00d96-881f-4691-bb03-84fec2a75a26","metadata":{"id":"77d00d96-881f-4691-bb03-84fec2a75a26"},"outputs":[],"source":["# Define the vocabulary as the dictionnary of tokens\n","vocab = {token:integer for integer,token in enumerate(all_words)}\n","\n"]},{"cell_type":"markdown","source":["### Question 1: give the Token ID of the token \"now\"?"],"metadata":{"id":"7GXW-PrN-oei"},"id":"7GXW-PrN-oei"},{"cell_type":"code","source":["# Write down your code here."],"metadata":{"id":"UqwG1AjLOS3A"},"id":"UqwG1AjLOS3A","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 2: what is the role of \"encode\" in the following class? What is the role of \"decode\"?"],"metadata":{"id":"C4T6sTo01Ke7"},"id":"C4T6sTo01Ke7"},{"cell_type":"markdown","source":["Write down your answer here."],"metadata":{"id":"hFUGXD-3OXuq"},"id":"hFUGXD-3OXuq"},{"cell_type":"code","execution_count":null,"id":"f531bf46-7c25-4ef8-bff8-0d27518676d5","metadata":{"id":"f531bf46-7c25-4ef8-bff8-0d27518676d5"},"outputs":[],"source":["class SimpleTokenizerV1:\n","    def __init__(self, vocab):\n","        self.str_to_int = vocab\n","        self.int_to_str = {i:s for s,i in vocab.items()}\n","\n","    def encode(self, text):\n","        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","\n","        preprocessed = [\n","            item.strip() for item in preprocessed if item.strip()\n","        ]\n","        ids = [self.str_to_int[s] for s in preprocessed]\n","        return ids\n","\n","    def decode(self, ids):\n","        text = \" \".join([self.int_to_str[i] for i in ids])\n","        # Replace spaces before the specified punctuations\n","        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","        return text"]},{"cell_type":"markdown","id":"c2950a94-6b0d-474e-8ed0-66d0c3c1a95c","metadata":{"id":"c2950a94-6b0d-474e-8ed0-66d0c3c1a95c"},"source":["- We can use the tokenizer to encode (that is, tokenize) texts into integers\n","- These integers can then be embedded (later) as input of/for the LLM"]},{"cell_type":"code","execution_count":null,"id":"647364ec-7995-4654-9b4a-7607ccf5f1e4","metadata":{"id":"647364ec-7995-4654-9b4a-7607ccf5f1e4"},"outputs":[],"source":["tokenizer = SimpleTokenizerV1(vocab)\n","\n","text = \"\"\"\"It's the last he painted, you know,\"\n","           Mrs. Gisburn said with pardonable pride.\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)\n","\n","# We can decode the integers back into text\n","print(tokenizer.decode(ids))"]},{"cell_type":"markdown","id":"4b821ef8-4d53-43b6-a2b2-aef808c343c7","metadata":{"id":"4b821ef8-4d53-43b6-a2b2-aef808c343c7"},"source":["## 3- Adding special context tokens"]},{"cell_type":"markdown","id":"9d709d57-2486-4152-b7f9-d3e4bd8634cd","metadata":{"id":"9d709d57-2486-4152-b7f9-d3e4bd8634cd"},"source":["**General \"special\" tokens**\n","\n","- It's useful to add some \"special\" tokens for unknown words and to denote the end of a text\n","- Some tokenizers use special tokens to help the LLM with additional context\n","- Some of these special tokens are\n","  - `[BOS]` (beginning of sequence) marks the beginning of text\n","  - `[EOS]` (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n","  - `[PAD]` (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n","- `[UNK]` to represent words that are not included in the vocabulary\n","\n","\n"]},{"cell_type":"markdown","source":["**GPT-2 \"special\" tokens**\n","\n","- Note that GPT-2 does not need any of these tokens mentioned above but only uses an `<|endoftext|>` token to reduce complexity\n","- The `<|endoftext|>` is analogous to the `[EOS]` token mentioned above\n","- GPT also uses the `<|endoftext|>` for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n","- GPT-2 does not use an `<UNK>` token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units\n","- We use the `<|endoftext|>` tokens between two independent sources of text\n"],"metadata":{"id":"6KCRpBs7JS5B"},"id":"6KCRpBs7JS5B"},{"cell_type":"markdown","id":"c661a397-da06-4a86-ac27-072dbe7cb172","metadata":{"id":"c661a397-da06-4a86-ac27-072dbe7cb172"},"source":["### Question 3: why does an error occur in the following cell?"]},{"cell_type":"markdown","source":["Write down your answer here."],"metadata":{"id":"eGOunUolOsQV"},"id":"eGOunUolOsQV"},{"cell_type":"code","execution_count":null,"id":"d5767eff-440c-4de1-9289-f789349d6b85","metadata":{"id":"d5767eff-440c-4de1-9289-f789349d6b85"},"outputs":[],"source":["tokenizer = SimpleTokenizerV1(vocab)\n","\n","text = \"Hello, do you like tea. Is this-- a test?\"\n","\n","#tokenizer.encode(text)"]},{"cell_type":"code","execution_count":null,"id":"ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f","metadata":{"id":"ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f"},"outputs":[],"source":["all_tokens = sorted(list(set(preprocessed)))\n","all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n","\n","vocab = {token:integer for integer,token in enumerate(all_tokens)}\n","\n","print(len(vocab.items()))\n","\n","for i, item in enumerate(list(vocab.items())[-5:]):\n","    print(item)"]},{"cell_type":"markdown","source":["### Question 4: modify the class \"Simple TokenizerV1\" to use the `<unk>` token when the token is not in the predefined vocabulary. The modified class will be named \"SimpleTokenizerV2\"."],"metadata":{"id":"fyi-R1xCK2tj"},"id":"fyi-R1xCK2tj"},{"cell_type":"code","source":["# Write down your code here.\n","\n","class SimpleTokenizerV2:\n","    def __init__(self, vocab):\n","        self.str_to_int = vocab\n","        self.int_to_str = { i:s for s,i in vocab.items()}\n","\n","    def encode(self, text):\n","        pass\n","        return ids\n","\n","    def decode(self, ids):\n","        pass\n","        return text"],"metadata":{"id":"H6CH8bo-Ox74"},"id":"H6CH8bo-Ox74","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"aa728dd1-9d35-4ac7-938f-d411d73083f6","metadata":{"id":"aa728dd1-9d35-4ac7-938f-d411d73083f6"},"source":["Let's try to tokenize text with the modified tokenizer:"]},{"cell_type":"code","execution_count":null,"id":"4133c502-18ac-4412-9f43-01caf4efa3dc","metadata":{"id":"4133c502-18ac-4412-9f43-01caf4efa3dc"},"outputs":[],"source":["tokenizer = SimpleTokenizerV2(vocab)\n","\n","text1 = \"Hello, do you like tea?\"\n","text2 = \"In the sunlit terraces of the palace.\"\n","\n","text = \" <|endoftext|> \".join((text1, text2))\n","\n","print(text)\n","\n","print(tokenizer.encode(text))\n","\n","print(tokenizer.decode(tokenizer.encode(text)))"]},{"cell_type":"markdown","id":"5c4ba34b-170f-4e71-939b-77aabb776f14","metadata":{"id":"5c4ba34b-170f-4e71-939b-77aabb776f14"},"source":["## 4- Byte-Pair encoding"]},{"cell_type":"markdown","id":"2309494c-79cf-4a2d-bc28-a94d602f050e","metadata":{"id":"2309494c-79cf-4a2d-bc28-a94d602f050e"},"source":["- GPT-2 used Byte-Pair encoding (BPE) as its tokenizer\n","- It allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n","- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n","- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n","- We are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library\n"]},{"cell_type":"code","execution_count":null,"id":"6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128","metadata":{"id":"6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128"},"outputs":[],"source":["tokenizer = tiktoken.get_encoding(\"gpt2\")"]},{"cell_type":"code","execution_count":null,"id":"5ff2cd85-7cfb-4325-b390-219938589428","metadata":{"id":"5ff2cd85-7cfb-4325-b390-219938589428"},"outputs":[],"source":["text = (\n","    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n","     \"of someunknownPlace.\"\n",")\n","\n","integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","\n","print(integers)\n","\n","strings = tokenizer.decode(integers)\n","\n","print(strings)"]},{"cell_type":"markdown","id":"abbd7c0d-70f8-4386-a114-907e96c950b0","metadata":{"id":"abbd7c0d-70f8-4386-a114-907e96c950b0"},"source":["## 5- Data sampling with a sliding window"]},{"cell_type":"markdown","id":"509d9826-6384-462e-aa8a-a7c73cd6aad0","metadata":{"id":"509d9826-6384-462e-aa8a-a7c73cd6aad0"},"source":["- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict.\n","- In the following sentence for instance, we want to predict `notebook` from the words occuring before `notebook`. We don't use the words occuring after it. The LLM can't acces words past the target.\n","- Example : The students opened their notebook when the lecture begins.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"848d5ade-fd1f-46c3-9e31-1426e315c71b","metadata":{"id":"848d5ade-fd1f-46c3-9e31-1426e315c71b"},"outputs":[],"source":["enc_text = tokenizer.encode(raw_text)\n","print(len(enc_text))"]},{"cell_type":"markdown","id":"cebd0657-5543-43ca-8011-2ae6bd0a5810","metadata":{"id":"cebd0657-5543-43ca-8011-2ae6bd0a5810"},"source":["- For each text chunk, we want the inputs and targets\n","- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"]},{"cell_type":"markdown","source":["### Question 5: what is the meaning of \"context_size\" in the following cell?"],"metadata":{"id":"ZrrpBXhjNyn0"},"id":"ZrrpBXhjNyn0"},{"cell_type":"markdown","source":["Write your answer here"],"metadata":{"id":"-vQ62ADPPSp1"},"id":"-vQ62ADPPSp1"},{"cell_type":"code","execution_count":null,"id":"dfbff852-a92f-48c8-a46d-143a0f109f40","metadata":{"id":"dfbff852-a92f-48c8-a46d-143a0f109f40"},"outputs":[],"source":["context_size = 4\n","\n","enc_sample = enc_text[50:]\n","\n","x = enc_sample[:context_size]\n","y = enc_sample[1:context_size+1]\n","\n","print(f\"x: {x}\")\n","print(f\"y:      {y}\")"]},{"cell_type":"markdown","id":"815014ef-62f7-4476-a6ad-66e20e42b7c3","metadata":{"id":"815014ef-62f7-4476-a6ad-66e20e42b7c3"},"source":["- One by one, the prediction would look like as follows:"]},{"cell_type":"code","execution_count":null,"id":"f57bd746-dcbf-4433-8e24-ee213a8c34a1","metadata":{"id":"f57bd746-dcbf-4433-8e24-ee213a8c34a1"},"outputs":[],"source":["for i in range(1, context_size+1):\n","    context = enc_sample[:i]\n","    desired = enc_sample[i]\n","\n","    print(context, \"---->\", desired)\n","    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]),\"\\n\")"]},{"cell_type":"markdown","id":"92ac652d-7b38-4843-9fbd-494cdc8ec12c","metadata":{"id":"92ac652d-7b38-4843-9fbd-494cdc8ec12c"},"source":["- Create dataset and dataloader that extract chunks from the input text dataset\n","- The dataloader iterates over the input dataset and returns the inputs and targets shifted by one"]},{"cell_type":"code","execution_count":null,"id":"74b41073-4c9f-46e2-a1bd-d38e4122b375","metadata":{"id":"74b41073-4c9f-46e2-a1bd-d38e4122b375"},"outputs":[],"source":["class GPTDatasetV1(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Tokenize the entire text\n","        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n","\n","        # Use a sliding window to chunk the book into overlapping sequences of max_length\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]"]},{"cell_type":"code","execution_count":null,"id":"5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e","metadata":{"id":"5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e"},"outputs":[],"source":["def create_dataloader_v1(txt, batch_size=4, max_length=256,\n","                         stride=128, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader"]},{"cell_type":"markdown","source":["### Question 6: test the data loader \"create_dataloader_v1\" with a batch size of 1, a context size of 4 and a stride of 1. Be sure to understand the content of a batch."],"metadata":{"id":"KJn7UdZGOrNI"},"id":"KJn7UdZGOrNI"},{"cell_type":"code","source":["# Write down your code here"],"metadata":{"id":"3kHmpGfFPt18"},"id":"3kHmpGfFPt18","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16","metadata":{"id":"b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16"},"source":["- We can also create batched outputs\n","- Note that we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting"]},{"cell_type":"code","execution_count":null,"id":"1916e7a6-f03d-4f09-91a6-d0bdbac5a58c","metadata":{"id":"1916e7a6-f03d-4f09-91a6-d0bdbac5a58c"},"outputs":[],"source":["dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n","\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)\n","print(\"Inputs:\\n\", inputs)\n","print(\"\\nTargets:\\n\", targets)"]},{"cell_type":"markdown","id":"2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1","metadata":{"id":"2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1"},"source":["## 6- Creating token embeddings"]},{"cell_type":"markdown","id":"1a301068-6ab2-44ff-a915-1ba11688274f","metadata":{"id":"1a301068-6ab2-44ff-a915-1ba11688274f"},"source":["- The data is already almost ready for an LLM\n","- But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n","- Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training\n","- Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):"]},{"cell_type":"code","execution_count":null,"id":"15a6304c-9474-4470-b85d-3991a49fa653","metadata":{"id":"15a6304c-9474-4470-b85d-3991a49fa653"},"outputs":[],"source":["input_ids = torch.tensor([2, 3, 5, 1])"]},{"cell_type":"markdown","id":"14da6344-2c71-4837-858d-dd120005ba05","metadata":{"id":"14da6344-2c71-4837-858d-dd120005ba05"},"source":["- For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3"]},{"cell_type":"markdown","source":["### Question 7: what is the role of \"torch.nn.Embedding\" in the following cell? How is it initialized?"],"metadata":{"id":"7eiirJUQEKv0"},"id":"7eiirJUQEKv0"},{"cell_type":"markdown","source":["Write your answer here."],"metadata":{"id":"VACPHnZvQgpX"},"id":"VACPHnZvQgpX"},{"cell_type":"code","execution_count":null,"id":"93cb2cee-9aa6-4bb8-8977-c65661d16eda","metadata":{"id":"93cb2cee-9aa6-4bb8-8977-c65661d16eda"},"outputs":[],"source":["vocab_size = 6\n","output_dim = 3\n","\n","torch.manual_seed(4598)\n","embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n","\n","print(embedding_layer.weight)"]},{"cell_type":"markdown","id":"4b0d58c3-83c0-4205-aca2-9c48b19fd4a7","metadata":{"id":"4b0d58c3-83c0-4205-aca2-9c48b19fd4a7"},"source":["- To convert a token with id 5 into a 3-dimensional vector, we do the following:"]},{"cell_type":"code","execution_count":null,"id":"e43600ba-f287-4746-8ddf-d0f71a9023ca","metadata":{"id":"e43600ba-f287-4746-8ddf-d0f71a9023ca"},"outputs":[],"source":["print(embedding_layer(torch.tensor([5])))"]},{"cell_type":"markdown","id":"c393d270-b950-4bc8-99ea-97d74f2ea0f6","metadata":{"id":"c393d270-b950-4bc8-99ea-97d74f2ea0f6"},"source":["## 7- Encoding word positions"]},{"cell_type":"markdown","id":"24940068-1099-4698-bdc0-e798515e2902","metadata":{"id":"24940068-1099-4698-bdc0-e798515e2902"},"source":["- Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence\n","- Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model"]},{"cell_type":"markdown","source":[" ### Question 8: create an embedding, named \"token_embedding_layer\", for the BytePair encoder into a 256-dimensional vector representation.\n","\n","Reminder: The BytePair encoder has a vocabulary size of 50,257"],"metadata":{"id":"oA9XSqjnGJyM"},"id":"oA9XSqjnGJyM"},{"cell_type":"code","source":["# Write down your code here"],"metadata":{"id":"KT-YMaAdRi3U"},"id":"KT-YMaAdRi3U","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" ### Question 9: use \"create_dataloader_v1\" to generate a batch of size 8 and a context size of 4. Embed the tokens of this batch with \"token_embedding_layer\"."],"metadata":{"id":"wq-O4c9aICF1"},"id":"wq-O4c9aICF1"},{"cell_type":"code","source":["max_length = 4\n","# Write down your code here"],"metadata":{"id":"ckSwZyWqSXxa"},"id":"ckSwZyWqSXxa","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"fe2ae164-6f19-4e32-b9e5-76950fcf1c9f","metadata":{"id":"fe2ae164-6f19-4e32-b9e5-76950fcf1c9f"},"source":["- GPT-2 uses absolute position embeddings, so we just create another embedding layer"]},{"cell_type":"markdown","source":[" ### Question 10: create an embedding layer \"pos_embedding_layer\" for position embeddings. Choose correctly the parameters of this embedding."],"metadata":{"id":"ntZ3blVySGNu"},"id":"ntZ3blVySGNu"},{"cell_type":"code","source":["# Write down your code here"],"metadata":{"id":"Ip3EvUrmSfNr"},"id":"Ip3EvUrmSfNr","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"c369a1e7-d566-4b53-b398-d6adafb44105","metadata":{"id":"c369a1e7-d566-4b53-b398-d6adafb44105"},"outputs":[],"source":["pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n","print(pos_embeddings.shape)\n","\n","print(pos_embeddings)"]},{"cell_type":"markdown","id":"870e9d9f-2935-461a-9518-6d1386b976d6","metadata":{"id":"870e9d9f-2935-461a-9518-6d1386b976d6"},"source":["- To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:"]},{"cell_type":"code","execution_count":null,"id":"b22fab89-526e-43c8-9035-5b7018e34288","metadata":{"id":"b22fab89-526e-43c8-9035-5b7018e34288"},"outputs":[],"source":["input_embeddings = token_embeddings + pos_embeddings\n","print(input_embeddings.shape)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}